{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Model Selection",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "3wzAqb-HLUkY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "\n",
        "from sklearn import metrics as skmetrics\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as dsets\n",
        "import torch.utils.data as Data\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4OKqn3igvzYH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = \"/content/drive/My Drive/Colab Notebooks/Ethics/\"\n",
        "def save_obj(obj, name ):\n",
        "    with open(path+ name + '.pkl', 'wb') as f:\n",
        "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "def load_obj(name ):\n",
        "    with open(path + name + '.pkl', 'rb') as f:\n",
        "        return pickle.load(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wePWRzBY0ppG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "privileged_groups = [{'sex': 1}]\n",
        "unprivileged_groups = [{'sex': 0}]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGA4umZR02L5",
        "colab_type": "text"
      },
      "source": [
        "Split Audult Dataset into training and test data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lblzYQ_Q07ZH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ADULT DATASET (saved features as for a reason aif360 was not working anymore)\n",
        "\n",
        "metadata = load_obj('adult_data')\n",
        "features = load_obj('features')\n",
        "labels = load_obj('labels')\n",
        "data = np.concatenate((features,labels),axis=1)\n",
        "\n",
        "\n",
        "def split_data(data, split_point):\n",
        "    assert split_point < 1 and split_point > 0\n",
        "    np.random.shuffle(data)\n",
        "    s = split_point\n",
        "    N = int(data.shape[0] * s)\n",
        "    X_train, y_train = data[:N][:,:-1], data[:N][:,-1]\n",
        "    X_test, y_test = data[N:][:,:-1], data[N:][:,-1]\n",
        "\n",
        "    return X_train, y_train , X_test, y_test\n",
        "\n",
        "def preprocess(X_train, X_test):\n",
        "\n",
        "    scale_orig = StandardScaler()\n",
        "    x_train = scale_orig.fit_transform(X_train)\n",
        "    x_test = scale_orig.transform(X_test)\n",
        "\n",
        "    return x_train, x_test\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8UA4ftgDr_Dt",
        "colab_type": "text"
      },
      "source": [
        "Create a Logistic Regression class with pytorch:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUZ_2tnniemT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LogisticRegression(torch.nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(LogisticRegression, self).__init__()\n",
        "        self.linear = torch.nn.Linear(input_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        outputs = torch.sigmoid(self.linear(x))\n",
        "        return outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8asauPU0Go_a",
        "colab_type": "text"
      },
      "source": [
        "Reweighing:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EU01TG2IBfrz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def reweighing(feat, lab, sens_class = 'sex'):\n",
        "    if sens_class == 'sex':\n",
        "        # Sex labels\n",
        "        features = feat[:,1]\n",
        "    elif sens_class == 'race':\n",
        "        features = feat[:,0]\n",
        "    else:\n",
        "        raise 'Not valid class name'\n",
        "    # Income Labels\n",
        "    WEIGHTS = {}\n",
        "    # W_(y,a) = count(Y=y) * count(A=a) / (count(Y=y, A=a) * N)\n",
        "    # In total 2 unique weights\n",
        "    for Y in range(2):\n",
        "        WEIGHTS[Y] = {}\n",
        "        for A in range(2):\n",
        "            NY = sum(lab.ravel() == Y)\n",
        "            NA = sum((features == A))\n",
        "            NYA = sum((lab.ravel() == Y) & (features == A))\n",
        "            WEIGHTS[Y][A] = {}\n",
        "            WEIGHTS[Y][A] = (NY * NA / (NYA * lab.shape[0]))\n",
        "\n",
        "    W = torch.zeros(lab.shape[0])\n",
        "    for i in range(len(lab)):\n",
        "        W[i] = WEIGHTS[lab[i]][features[i]]\n",
        "\n",
        "    return W\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdWf5TkbjEdA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "GPU = True\n",
        "device_idx = 0\n",
        "if GPU:\n",
        "    device = torch.device(\"cuda:\" + str(device_idx) if torch.cuda.is_available() else \"cpu\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    \n",
        "BATCH_SIZE = 32\n",
        "learning_rate = 0.0001\n",
        "\n",
        "\n",
        "if device == 0:\n",
        "    num_workers = 2\n",
        "else:\n",
        "    num_workers = 0\n",
        "\n",
        "\n",
        "def get_loader(x_train, y_train, W):\n",
        "    train_dataset = Data.TensorDataset(torch.tensor(x_train).float(), \n",
        "                                      torch.Tensor(y_train).float(), \n",
        "                                      W.float())\n",
        "    \n",
        "    return Data.DataLoader(dataset=train_dataset,\n",
        "                           batch_size=BATCH_SIZE, \n",
        "                           shuffle=True, num_workers=num_workers)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsWwFZ5kpzrk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Split data \n",
        "X_train, y_train , X_test, y_test = split_data(data, 0.6)\n",
        "# Preprocess data\n",
        "x_train, x_test = preprocess(X_train, X_test)\n",
        "# Get weights\n",
        "W = reweighing(X_train, y_train)\n",
        "# \n",
        "loader_train = get_loader(x_train, y_train, W)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9nPhcQMsz3f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 937
        },
        "outputId": "b078b0c9-b214-4b45-cbab-9d0d567e7ab5"
      },
      "source": [
        "reg_lambda = 48\n",
        "num_splits = 10\n",
        "male_tpr = []\n",
        "male_tnr = []\n",
        "female_tpr = []\n",
        "female_tnr = []\n",
        "ACC = []\n",
        "TPR, TNR = [], []\n",
        "for split in range(num_splits):\n",
        "    # Split data \n",
        "    X_train, y_train , X_test, y_test = split_data(data, 0.7)\n",
        "    # Preprocess data\n",
        "    x_train, x_test = preprocess(X_train, X_test)\n",
        "    # Get weights\n",
        "    W = reweighing(X_train, y_train)\n",
        "    # \n",
        "    loader_train = get_loader(x_train, y_train, W)\n",
        "    model = LogisticRegression_torch(x_train.shape[1], 1)\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=0.0001)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        train_loss = 0.0\n",
        "        for i, (x, y, w) in enumerate(loader_train):\n",
        "        # Converting inputs and labels to Variable\n",
        "\n",
        "            inputs = Variable(x.to(device))\n",
        "            labels = Variable(y.to(device))\n",
        "\n",
        "            # Clear gradient buffers because we don't want any gradient \n",
        "            # from previous epoch to carry forward, dont want to cummulate gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # get output from the model, given the inputs\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Regularization\n",
        "            reg = 0\n",
        "            for param in model.parameters():\n",
        "                reg += 0.5 * (param ** 2).mean()\n",
        "            \n",
        "            # criterion\n",
        "            criterion = torch.nn.BCELoss(weight=w, reduction='sum')\n",
        "            # get loss for the predicted output\n",
        "            loss = criterion(outputs.reshape(outputs.shape[0]), labels) + \\\n",
        "                reg_lambda * reg\n",
        "            \n",
        "                \n",
        "            train_loss += loss.item()\n",
        "            \n",
        "            # get gradients w.r.t to parameters\n",
        "            loss.backward()\n",
        "            \n",
        "            # update parameters\n",
        "            optimizer.step()\n",
        "        \n",
        "\n",
        "        print('epoch [{}/{}], Training loss:{:.6f}'.format(\n",
        "            epoch + 1, \n",
        "            epochs, \n",
        "            train_loss / len(loader_train.dataset)))\n",
        "        \n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        out = model(Variable(torch.Tensor(x_test).to(device))).detach().cpu()\n",
        "        pred = (out >= 0.5).int().numpy().squeeze()\n",
        "        accuracy = sum((y_test == pred))/len(y_test)\n",
        "     \n",
        "    # Men \n",
        "    male = y_test[((X_test[:,1] == 1.0))]\n",
        "    male_pred = pred[((X_test[:,1] == 1.0))]\n",
        "    male_proba = out[((X_test[:,1] == 1.0))]\n",
        "\n",
        "    # Female\n",
        "    female = y_test[((X_test[:,1] == 0.0))]\n",
        "    female_pred = pred[((X_test[:,1] == 0.0))]\n",
        "    female_proba = out[((X_test[:,1] == 0.0))]\n",
        "\n",
        "    ACC.append(accuracy)\n",
        "\n",
        "    # GET TPR and TNR for each unique group\n",
        "    tn, fp, fn, tp = CM(male, male_pred).ravel()\n",
        "    male_tpr.append(tp/(tp+fn)) \n",
        "    male_tnr.append(tn/(tn+fp))\n",
        "    \n",
        "\n",
        "\n",
        "    tn, fp, fn, tp = CM(female, female_pred).ravel()\n",
        "    female_tpr.append(tp/(tp+fn)) \n",
        "    female_tnr.append(tn/(tn+fp))\n",
        "\n",
        "    TPR.append(abs(male_tpr[split] - female_tpr[split]))\n",
        "    TNR.append(abs(male_tnr[split] - female_tnr[split]))\n",
        "\n",
        "print('\\nAccuracy:')\n",
        "print('Mean:', np.mean(ACC), 'S.D.:', np.std(ACC))\n",
        "print('\\nMale:')\n",
        "print('Mean TPR:', np.mean(male_tpr), 'S.D.:', np.std(male_tpr),\n",
        "      'Mean TNR:', np.mean(male_tnr), 'S.D.:', np.std(male_tnr))\n",
        "print('\\nFemale:')\n",
        "print('Mean TPR:', np.mean(female_tpr), 'S.D.:', np.std(female_tpr),\n",
        "      'Mean TNR:', np.mean(female_tnr), 'S.D.:', np.std(female_tnr))\n",
        "print('\\n|TPR_m - TPR_f|:')\n",
        "print('Mean :', np.mean(TPR), 'S.D.:', np.std(TPR))\n",
        "print('\\n|TNR_m - TNR_f|:')\n",
        "print('Mean :', np.mean(TNR), 'S.D.:', np.std(TNR))\n"
      ],
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch [1/4], Training loss:0.636081\n",
            "epoch [2/4], Training loss:0.617128\n",
            "epoch [3/4], Training loss:0.615116\n",
            "epoch [4/4], Training loss:0.614794\n",
            "epoch [1/4], Training loss:0.656065\n",
            "epoch [2/4], Training loss:0.618991\n",
            "epoch [3/4], Training loss:0.616132\n",
            "epoch [4/4], Training loss:0.615814\n",
            "epoch [1/4], Training loss:0.637988\n",
            "epoch [2/4], Training loss:0.619231\n",
            "epoch [3/4], Training loss:0.617285\n",
            "epoch [4/4], Training loss:0.616902\n",
            "epoch [1/4], Training loss:0.669477\n",
            "epoch [2/4], Training loss:0.618132\n",
            "epoch [3/4], Training loss:0.615009\n",
            "epoch [4/4], Training loss:0.614722\n",
            "epoch [1/4], Training loss:0.656785\n",
            "epoch [2/4], Training loss:0.619256\n",
            "epoch [3/4], Training loss:0.616145\n",
            "epoch [4/4], Training loss:0.615804\n",
            "epoch [1/4], Training loss:0.680877\n",
            "epoch [2/4], Training loss:0.619766\n",
            "epoch [3/4], Training loss:0.615284\n",
            "epoch [4/4], Training loss:0.614762\n",
            "epoch [1/4], Training loss:0.653787\n",
            "epoch [2/4], Training loss:0.619119\n",
            "epoch [3/4], Training loss:0.616107\n",
            "epoch [4/4], Training loss:0.615504\n",
            "epoch [1/4], Training loss:0.631776\n",
            "epoch [2/4], Training loss:0.617075\n",
            "epoch [3/4], Training loss:0.615656\n",
            "epoch [4/4], Training loss:0.615453\n",
            "epoch [1/4], Training loss:0.665540\n",
            "epoch [2/4], Training loss:0.619744\n",
            "epoch [3/4], Training loss:0.616310\n",
            "epoch [4/4], Training loss:0.615925\n",
            "epoch [1/4], Training loss:0.651747\n",
            "epoch [2/4], Training loss:0.618337\n",
            "epoch [3/4], Training loss:0.615466\n",
            "epoch [4/4], Training loss:0.615045\n",
            "\n",
            "Accuracy:\n",
            "Mean: 0.7097659182419982 S.D.: 0.004430572782616282\n",
            "\n",
            "Male:\n",
            "Mean TPR: 0.7023906509197928 S.D.: 0.009238720702590597 Mean TNR: 0.7184267956901752 S.D.: 0.008861551159756795\n",
            "\n",
            "Female:\n",
            "Mean TPR: 0.7053561398114399 S.D.: 0.015260475457508797 Mean TNR: 0.7017233715032074 S.D.: 0.008527849566979124\n",
            "\n",
            "|TPR_m - TPR_f|:\n",
            "Mean : 0.009770816645080416 S.D.: 0.009318104652972477\n",
            "\n",
            "|TNR_m - TNR_f|:\n",
            "Mean : 0.016703424186967818 S.D.: 0.007939379854222085\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}